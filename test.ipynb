{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from DataUtil import *\n",
    "from my_model_p2 import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def save_data_to_pkl(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def load_data_from_pkl(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "afqmc_benchmarkReader= AFQMCBenchmarkReader(data_path=\"AFQMC\")\n",
    "train_sentence_pairs, train_scores, dev_sentence_pairs, dev_scores = afqmc_benchmarkReader.get_data()\n",
    "\n",
    "\n",
    "bert_file_path = \"bert-chinese\"\n",
    "external_embed_dim = 200\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_file_path)\n",
    "bert_model = BertModel.from_pretrained(bert_file_path).to(device)\n",
    "\n",
    "sentences_dict = load_data_from_pkl('AFQMC/AFQMC_sentence_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lcqmc_train_Dataset = STSDataset(train_sentence_pairs, train_scores, tokenizer, sentences_dict)\n",
    "lcqmc_dev_Dataset = STSTestDataset(dev_sentence_pairs, dev_scores, tokenizer, sentences_dict)\n",
    "\n",
    "lcqmc_train_loader = DataLoader(lcqmc_train_Dataset, batch_size=64, shuffle=True)\n",
    "lcqmc_dev_loader = DataLoader(lcqmc_dev_Dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "print(\"Model loading...\")\n",
    "myModel = SimilarityModel(bert_model).to(device)# 确保模型在GPU上\n",
    "# myModel.load_state_dict(torch.load('/data/szj/sts/transformers/SZJ_Model/STSModel.pth'))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        # 使用 tqdm 包裹数据加载器，显示进度条\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, scores, sentence1_words_embeddings, sentence2_words_embeddings = batch\n",
    "            input_ids = inputs['input_ids'].squeeze(1).to(device)  # 移动到GPU\n",
    "            attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "            scores = scores.float()\n",
    "            scores = scores.to(device)  # 移动到GPU\n",
    "            scores = scores.unsqueeze(1)\n",
    "            sentence1_words_embeddings = sentence1_words_embeddings.float().to(device)\n",
    "            sentence2_words_embeddings = sentence2_words_embeddings.float().to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask,\n",
    "                            sentence1_words_embeddings,\n",
    "                            sentence2_words_embeddings)\n",
    "            loss = criterion(outputs, scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "        evaluate_model_p(myModel, test_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs, scores, sentence1_words_embeddings, sentence2_words_embeddings = batch\n",
    "            input_ids = inputs['input_ids'].squeeze(1).to(device)  # 移动到GPU\n",
    "            attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "            scores = scores.to(device)  # 移动到GPU\n",
    "            sentence1_words_embeddings = sentence1_words_embeddings.to(device)\n",
    "            sentence2_words_embeddings = sentence2_words_embeddings.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            sentence1_words_embeddings=sentence1_words_embeddings,\n",
    "                            sentence2_words_embeddings=sentence2_words_embeddings)\n",
    "            loss = criterion(outputs, scores)\n",
    "            total_loss += loss.item()\n",
    "    test_loss = total_loss / len(data_loader)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    return test_loss\n",
    "\n",
    "def evaluate_model_p(model, data_loader):\n",
    "    model.eval()\n",
    "    ture_num = 0\n",
    "    total_num = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs, scores, sentence1_words_embeddings, sentence2_words_embeddings = batch\n",
    "            input_ids = inputs['input_ids'].squeeze(1).to(device)  # 移动到GPU\n",
    "            attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "            scores = scores.to(device)\n",
    "            scores = scores.unsqueeze(1)  # 移动到GPU\n",
    "            sentence1_words_embeddings = sentence1_words_embeddings.float().to(device)\n",
    "            sentence2_words_embeddings = sentence2_words_embeddings.float().to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask,\n",
    "                            sentence1_words_embeddings,\n",
    "                            sentence2_words_embeddings)\n",
    "            for i in range(len(outputs)):\n",
    "                if float(outputs[i]) > 0.8:\n",
    "                    score = 1\n",
    "                else:\n",
    "                    score = 0\n",
    "                if int(scores[i]) == score:\n",
    "                    ture_num += 1\n",
    "                total_num += 1\n",
    "    print(str(ture_num)+'/'+str(total_num))\n",
    "    print('Accuracy: '+str(ture_num/total_num))\n",
    "    accuracy_list.append(str(ture_num)+'/'+str(total_num))\n",
    "\n",
    "# Training the model\n",
    "accuracy_list = []\n",
    "print(\"Training...\")\n",
    "optimizer = optim.Adam(myModel.parameters(), lr=1e-5)\n",
    "train_model(myModel, lcqmc_train_loader, lcqmc_dev_loader,criterion, optimizer, epochs=5)\n",
    "# train_model(myModel, lcqmc_train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "# Evaluating the model\n",
    "dev_loss = evaluate_model_p(myModel, lcqmc_dev_loader)\n",
    "print(f\"Dev Loss: {dev_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(myModel.parameters(), lr=1e-5)\n",
    "train_model(myModel, lcqmc_train_loader, lcqmc_dev_loader,criterion, optimizer, epochs=5)\n",
    "# train_model(myModel, lcqmc_train_loader, criterion, optimizer, epochs=1)\n",
    "\n",
    "# Evaluating the model\n",
    "dev_loss = evaluate_model_p(myModel, lcqmc_dev_loader)\n",
    "print(f\"Dev Loss: {dev_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model_p(model, data_loader):\n",
    "    model.eval()\n",
    "    ture_num = 0\n",
    "    total_num = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs, scores, sentence1_words_embeddings, sentence2_words_embeddings = batch\n",
    "            input_ids = inputs['input_ids'].squeeze(1).to(device)  # 移动到GPU\n",
    "            attention_mask = inputs['attention_mask'].squeeze(1).to(device)\n",
    "            scores = scores.to(device)\n",
    "            scores = scores.unsqueeze(1)  # 移动到GPU\n",
    "            sentence1_words_embeddings = sentence1_words_embeddings.float().to(device)\n",
    "            sentence2_words_embeddings = sentence2_words_embeddings.float().to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask,\n",
    "                            sentence1_words_embeddings,\n",
    "                            sentence2_words_embeddings)\n",
    "            for i in range(len(outputs)):\n",
    "                if float(outputs[i]) > 0.3:\n",
    "                    score = 1\n",
    "                else:\n",
    "                    score = 0\n",
    "                if int(scores[i]) == score:\n",
    "                    ture_num += 1\n",
    "                total_num += 1\n",
    "    print(str(ture_num)+'/'+str(total_num))\n",
    "    print('Accuracy: '+str(ture_num/total_num))\n",
    "    accuracy_list.append(str(ture_num)+'/'+str(total_num))\n",
    "\n",
    "evaluate_model_p(myModel, lcqmc_dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BCEmbedding import EmbeddingModel\n",
    "embedding_model = EmbeddingModel(model_name_or_path=\"/data/coding/Short-Text-Similarity/bce-embedding\")\n",
    "emb = embedding_model.encode([\"手机\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_data_to_pkl(data, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def load_data_from_pkl(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "sentences_dict1 = load_data_from_pkl('/data/coding/Short-Text-Similarity/AFQMC_sentence_bce_dict_1.pkl')\n",
    "sentences_dict2 = load_data_from_pkl('/data/coding/Short-Text-Similarity/AFQMC_sentence_bce_dict_2.pkl')\n",
    "sentences_dict3 = load_data_from_pkl('/data/coding/Short-Text-Similarity/AFQMC_sentence_bce_dict_3.pkl')\n",
    "sentences_dict4 = load_data_from_pkl('/data/coding/Short-Text-Similarity/AFQMC_sentence_bce_dict_4.pkl')\n",
    "sentences_dict = {**sentences_dict1, **sentences_dict2, **sentences_dict3, **sentences_dict4}\n",
    "print(len(sentences_dict1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_pkl(sentences_dict,\"AFQMC_sentence_bce_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences_dict1))\n",
    "print(len(sentences_dict2))\n",
    "print(len(sentences_dict3))\n",
    "print(len(sentences_dict4))\n",
    "print(len(sentences_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataUtil import *\n",
    "LCQMCReader = LCQMCBenchmarkReader('LCQMC')\n",
    "train_sentence_pairs, train_scores, dev_sentence_pairs, dev_scores, test_sentence_pairs, test_scores = LCQMCReader.get_data()\n",
    "\n",
    "# sentence_list = LCQMC_train_sentence_pairs+LCQMC_dev_sentence_pairs+LCQMC_test_sentence_pairs\n",
    "sentence_list = train_sentence_pairs  + dev_sentence_pairs + test_sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from BCEmbedding import EmbeddingModel\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "embedding_model = EmbeddingModel(model_name_or_path=\"/data/coding/Short-Text-Similarity/bce-embedding\")\n",
    "word_vector_length = 768\n",
    "# 保证 n 为 50\n",
    "n = 50\n",
    "sentence_dict={}\n",
    "for sentence1,sentence2 in tqdm(sentence_list):\n",
    "    seg_list1 = list(jieba.cut(remove_punctuation(sentence1), cut_all=True))\n",
    "    # 获取模型中每个词的向量\n",
    "    sentence_embedding1 = []\n",
    "    embeddings =[]\n",
    "    for word in seg_list1:\n",
    "        try:\n",
    "            embeddings.append(embedding_model.encode([word])[0])\n",
    "        except:\n",
    "            embeddings.append(np.zeros(word_vector_length))\n",
    "    if len(sentence_embedding1) > n:\n",
    "        # 如果超过 50 个词，只取前 50 个词\n",
    "        sentence_embedding1 = sentence_embedding1[:n]\n",
    "    else:\n",
    "        # 如果少于 50 个词，用零向量补充\n",
    "        padding_length = n - len(sentence_embedding1)\n",
    "        sentence_embedding1.extend([np.zeros(word_vector_length)] * padding_length)\n",
    "    # 最终的句子向量矩阵为 50 x 200\n",
    "    sentence_matrix1 = np.array(sentence_embedding1)\n",
    "    sentence_matrix1 = csr_matrix(sentence_matrix1)\n",
    "    sentence_dict[remove_punctuation(sentence1)] = sentence_matrix1\n",
    "\n",
    "    seg_list2 = list(jieba.cut(remove_punctuation(sentence2), cut_all=True))\n",
    "    # 获取模型中每个词的向量\n",
    "    sentence_embedding2 = []\n",
    "    embeddings =[]\n",
    "    for word in seg_list2:\n",
    "        try:\n",
    "            embeddings.append(embedding_model.encode([word])[0])\n",
    "        except:\n",
    "            embeddings.append(np.zeros(word_vector_length))\n",
    "    if len(sentence_embedding2) > n:\n",
    "        # 如果超过 50 个词，只取前 50 个词\n",
    "        sentence_embedding2 = sentence_embedding2[:n]\n",
    "    else:\n",
    "        # 如果少于 50 个词，用零向量补充\n",
    "        padding_length = n - len(sentence_embedding2)\n",
    "        sentence_embedding2.extend([np.zeros(word_vector_length)] * padding_length)\n",
    "    # 最终的句子向量矩阵为 50 x 200\n",
    "    sentence_matrix2 = np.array(sentence_embedding2)\n",
    "    sentence_matrix2 = csr_matrix(sentence_matrix2)\n",
    "    sentence_dict[remove_punctuation(sentence2)] = sentence_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_pkl(sentence_dict, 'LCQMC_sentence_bce_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
